- name: "[KAFKA]: High CPU Usage Detected. System Under Heavy Load."
  query: max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{pod="kafka-0"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_KAFKA_001"
  description: "High CPU usage in Kafka may impact data processing and system performance."
  annotations: {}
  severity: warning

- name: "[VALKEY]: High CPU Usage Detected. System Under Heavy Load."
  query: sum by()(max by (label_system_infra,pod) (rate(container_cpu_usage_seconds_total{pod="valkey-dedup-primary-0"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"})) ) + sum by() (max by (label_system_infra,pod) (rate(container_cpu_usage_seconds_total{pod="valkey-denorm-primary-0"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_VALKEY_002"
  description: "High CPU usage in Valkey may lead to slow key access and degraded caching performance."
  annotations: {}
  severity: warning

- name: "[DRUID HISTORICAL]: High CPU Usage Detected. System Under Heavy Load."
  query: max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='druid-raw-historicals'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_HISTORICAL_003"
  description: "High CPU usage in Druid Historicals can slow down query processing."
  annotations: {}
  severity: warning

- name: "[DRUID INDEXER]: High CPU Usage Detected. System Under Heavy Load."
  query: max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='druid-raw-indexers'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_INDEXER_004"
  description: "High CPU usage in Druid Indexers can delay data ingestion and impact real-time data availability."
  annotations: {}
  severity: warning

- name: "[DRUID OVERLORD]: High CPU Usage Detected. System Under Heavy Load."
  query: max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='druid-raw-overlords'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_OVERLORD_005"
  description: "High CPU usage in Druid Overlord can delay task assignment affecting ingestion task scheduling."
  annotations: {}
  severity: warning

- name: "[DRUID BROKER]: High CPU Usage Detected. System Under Heavy Load."
  query: max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='druid-raw-brokers'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_BROKER_006"
  description: "High CPU usage in Druid Brokers can degrade query routing and aggregation affecting end-user query experience."
  annotations: {}
  severity: warning

- name: "[PROMETHEUS]: High CPU Usage Detected. System Under Heavy Load."
  query: max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='prometheus'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_PROMETHEUS_007"
  description: "High CPU usage in Prometheus can delay metric collection and alert evaluations,."
  annotations: {}
  severity: warning


- name: "[GRAFANA]: High CPU Usage Detected. System Under Heavy Load."
  query: max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='grafana'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_GRAFANA_008"
  description: "High CPU usage in Grafana will impact the alert rules monitoring and visualization."
  annotations: {}
  severity: warning


- name: "[POSTGRES]: High CPU Usage Detected. System Under Heavy Load."
  query: max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='postgresql'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_POSTGRES_010"
  description: "High CPU usage in PostgreSQL can lead to slow query performance and potential application timeouts affecting data access and overall system responsiveness."
  annotations: {}
  severity: warning


- name: "[API SERVICE]: High CPU Usage Detected. System Under Heavy Load."
  query: max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='dataset-api'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_API_SVC_011"
  description: "High CPU usage in API service can cause slower response times failed requests potentially affecting service availability."
  annotations: {}
  severity: warning

- name: "[SECOR BACKUP]: High CPU Usage Detected. System Under Heavy Load."
  query: max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='dataset-api'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_SECOR_012"
  description: "High CPU usage in Secor can delay Kafka topic backups to cloud storage increasing the risk of data loss during outages or failures."
  annotations: {}
  severity: warning

- name: "[KAFKA MESSAGE EXPORTER]: High CPU Usage Detected. System Under Heavy Load."
  query:  max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='kafka-message-exporter'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_KAFKA_MSG_EXPORTER_013"
  description: "High CPU usage in Kafka Message Exporter can result in delayed or missing metric exports to Prometheus impacting monitoring accuracy."
  annotations: {}
  severity: warning

- name: "[UNIFIED PIPELINE]: High CPU Usage Detected. System Under Heavy Load."
  query: sum by()(max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='unified-pipeline-jobmanager'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"})))) + sum by() (max without(label_system_infra, pod)(max by (pod) (rate(container_cpu_usage_seconds_total{container='unified-pipeline-jobmanager'}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"})))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_UNIFIED_PIPELINE_014"
  description: "High CPU usage in Unified Pipeline can delay data processing."
  annotations: {}
  severity: warning

- name: "[KAFKA]: High Memory Usage Detected. System Could Become Unstable."
  query: max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{pod="kafka-0"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_KAFKA_015"
  description: "High memory usage in Kafka can degrade broker performance delay event streaming and impact the real-time processing of data."
  annotations: {}
  severity: warning

- name: "[VALKEY]: High Memory Usage Detected. System Could Become Unstable."
  query: sum by()(max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{pod="valkey-dedup-primary-0"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"})))) + sum by()(max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{pod="valkey-denorm-primary-0"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"})))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_VALKEY_016"
  description: "High memory usage in Valkey can cause increased latency eviction of keys and potential unavailability of cached data."
  annotations: {}
  severity: warning


- name: "[DRUID_HISTORICAL]: High Memory Usage Detected. System Could Become Unstable."
  query: max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{container="druid-raw-historicals"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_HISTORICAL_017"
  description: "High memory usage in Druid Historicals can slow down query performance and affect data retrieval."
  annotations: {}
  severity: warning

- name: "[DRUID INDEXER]: High Memory Usage Detected. System Could Become Unstable."
  query: max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{container="druid-raw-indexers"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_INDEXER_018"
  description: "High memory usage in Druid Indexers can delay data ingestion and impact the freshness of real-time analytics."
  annotations: {}
  severity: warning

- name: "[DRUID OVERLORD]: High Memory Usage Detected. System Could Become Unstable."
  query: max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{container="druid-raw-overlords"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_OVERLORD_019"
  description: "High memory usage in Overlord can disrupt task coordination causing delays in ingestion jobs and affecting data availability"
  annotations: {}
  severity: warning

- name: "[DRUID BROKER]: High Memory Usage Detected. System Could Become Unstable."
  query: max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{container="druid-raw-brokers"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_BROKER_020"
  description: "High memory usage in Brokers can slow the query performance."
  annotations: {}
  severity: warning

- name: "[PROMETHEUS]: High Memory Usage Detected. System Could Become Unstable."
  query: max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{container="prometheus"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_PROMETHEUS_021"
  description: "High memory usage in Prometheus can slow down metric scraping and increase query latency leading to delayed alerting."
  annotations: {}
  severity: warning

- name: "[GRAFANA]: High Memory Usage Detected. System Could Become Unstable."
  query: max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{container="grafana"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_GRAFANA_022"
  description: "High memory usage in Grafana can delay alert evaluations and impact notification delivery."
  annotations: {}
  severity: warning



- name: "[API SERVICE]: High Memory Usage Detected. System Could Become Unstable."
  query: max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{container="dataset-api"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"}))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_API_SVC_024"
  description: "High memory usage in the API service can lead to slower response times and possible crashes."
  annotations: {}
  severity: warning

- name: "[UNIFIED PIPELINE]: High Memory Usage Detected. System Could Become Unstable."
  query: sum by()(max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{container="unified-pipeline-jobmanager"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"})))) + sum by()(max without(label_system_infra, pod) ( max by (pod) (avg_over_time(container_memory_usage_bytes{container="unified-pipeline-jobmanager"}[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"})))) * 100
  operator: gt
  threshold: [90]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_UNIFIED_PIPELINE_025"
  description: "High memory usage in the Unified Pipeline can cause task failures and slow down data processing."
  annotations: {}
  severity: warning

- name: "[KAFKA]: System has restarted. May Affect Ingestion."
  query: increase(kube_pod_container_status_restarts_total{container="kafka",pod="kafka-0"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_KAFKA_026"
  description: "Frequent Kafka restarts can disrupt data processing pipelines leading to data loss and duplication."
  annotations: {}
  severity: critical


- name: "[DRUID HISTORICALS]: System has Restarted. May Affect Querying."
  query: increase(kube_pod_container_status_restarts_total{container="druid-raw-historicals"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_HISTORICAL_028"
  description: "Frequent restarts can delay access to historical segments slowing down query performance."
  annotations: {}
  severity: critical

- name: "[DRUID INDEXER]: System has Restarted. May Affect Querying."
  query: increase(kube_pod_container_status_restarts_total{container="druid-raw-indexers"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_INDEXER_029"
  description: "Frequent restarts can interrupt ingestion jobs causing delays in processing incoming data."
  annotations: {}
  severity: critical


- name: "[DRUID OVERLORD]: System has Restarted. May Affect Querying."
  query: increase(kube_pod_container_status_restarts_total{container="druid-raw-overlords"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_OVERLORD_030"
  description: "Frequent restarts can disrupt task management and ingestion coordination leading to failures in data indexing."
  annotations: {}
  severity: critical


- name: "[DRUID BROKER]: System has Restarted. May Affect Querying."
  query: increase(kube_pod_container_status_restarts_total{container="druid-raw-brokers"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_BROKER_031"
  description: "Frequent restarts can cause query routing issues and degrade query performance"
  annotations: {}
  severity: critical


- name: "[DRUID CORDINATOR]: System has Restarted. May Affect Querying."
  query: increase(kube_pod_container_status_restarts_total{container="druid-raw-coordinators"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_DRUID_COORDINATOR_032"
  description: "Frequent restarts can delay segment management and load balancing across historicals."
  annotations: {}
  severity: critical


- name: "[API SERVICE]: System has restarted. May Affect API Availability."
  query: increase(kube_pod_container_status_restarts_total{container="dataset-api"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_API_SVC_033"
  description: "Frequent restarts can disrupt request handling and cause intermittent failures."
  annotations: {}
  severity: critical


- name: "[PROMETHEUS]: System has restarted. May Affect Monitoring."
  query: increase(kube_pod_container_status_restarts_total{container="prometheus"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_PROMETHEUS_034"
  description: "Frequent restarts of Prometheus can disrupt metric scraping and delay alert evaluations."
  annotations: {}
  severity: critical


- name: "[GRAFANA]: System has restarted. May Affect Monitoring."
  query: increase(kube_pod_container_status_restarts_total{container="grafana"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_GRAFANA_035"
  description: "Frequent Grafana restarts may interrupt alert monitoring and impact notification delivery."
  annotations: {}
  severity: critical

- name: "[LOKI]: System has restarted. May Affect Monitoring."
  query: avg(increase(kube_pod_container_status_restarts_total{container="loki"}[$__range]))
  operator: gt
  threshold: [3]  
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_LOKI_036"
  description: "Frequent Loki restarts can cause gaps in log ingestion and querying."
  annotations: {}
  severity: critical


- name: "[WEB CONSOLE]: System has restarted. May Affect Data Analytics View in Web Console."
  query: increase(kube_pod_container_status_restarts_total{container="management-console"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_WEB_CONSOLE_040"
  description: "Frequent web console restarts can disrupt user interactions cause session timeours and visualize datasets."
  annotations: {}
  severity: critical


- name: "[POSTGRES]: System has restarted. May Affect Database Connectivity."
  query: increase(kube_pod_container_status_restarts_total{container="postgresql"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_POSTGRES_041"
  description: "Frequent PostgreSQL restarts can interrupt database operations, leading to increased query failures and degraded system performance."
  annotations: {}
  severity: critical


- name: "[PROMTAIL]: System has restarted. May Affect Log Ingestion."
  query: increase(kube_pod_container_status_restarts_total{container="promtail"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_PROMTAIL_042"
  description: "Frequent Promtail restarts may result in missed or delayed log forwarding to Loki."
  annotations: {}
  severity: critical


- name: "[KEYCLOAK]: System has restarted. May Affect User Authentication and Login Flow"
  query: increase(kube_pod_container_status_restarts_total{container="keycloak"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_KEYCLOAK_043"
  description: "Frequent Keycloak restarts can disrupt authentication and authorization flows causing login failures and access issues across integrated services."
  annotations: {}
  severity: critical

- name: "[SUPERSET]: System has restarted. May Affect Superset Dashboard Access "
  query: increase(kube_pod_container_status_restarts_total{container="superset"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_SUPERSET_044"
  description: "Frequent Superset restarts may lead to dashboard unavailability impacting access to analytical insights."
  annotations: {}
  severity: critical

- name: "[S3 EXPORTER]: System has restarted. May Affect Data export to S3."
  query: increase(kube_pod_container_status_restarts_total{container="s3-exporter"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_S3_EXPORTER_045"
  description: "Frequent restarts of the S3 Exporter may interrupt data exports to object storage leading to incomplete backups or delays in data archival."
  annotations: {}
  severity: critical


- name: "[VOLUME AUTOSCALER]: System has restarted. Automatic Volume Scaling May Be Disrupted."
  query: increase(kube_pod_container_status_restarts_total{container="volume-autoscaler"}[$__range])
  operator: gt
  threshold: [3]
  category: "Infra"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "INFRA_VOLUME_AUTOSCALER_046"
  description: "Frequent restarts or failures of the Volume Autoscaler can delay persistent volume resizing."
  annotations: {}
  severity: critical

- name: "[PERSISTENT VOLUMES]: Failed to automatically expand storage volume."
  query: volume_autoscaler_resize_failure_total
  operator: gt
  threshold: [0]
  category: "Storage"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "STORAGE_PV_003"
  description: "Failed Persistent Volume (PV) expansion can lead to storage bottlenecks, affecting data ingestion, processing, and service reliability."
  annotations: {}
  severity: critical

- name: "[VELERO]:Kubernetes cluster backup not found."
  query: sum(increase(velero_backup_failure_total[$__range]))
  operator: gt
  threshold: [0]
  category: "Storage"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "STORAGE_VELERO_008"
  description: "Missing Velero backups in the Kubernetes cluster can risk data loss and hinder recovery during outages or failures."
  annotations: {}
  severity: critical

- name: "[VALKEY]: Detected higher memory usage than expected."
  query: sum(sum_over_time(redis_memory_used_bytes[$__range]) / sum_over_time(redis_memory_max_bytes[$__range])) * 100
  operator: gt
  threshold: [80]
  category: "Processing"
  frequency: 5m
  interval: 5m
  labels: 
    alert_code: "PROCESS_VALKEY_012"
  description: "High memory usage in Valkey can slow down key retrievals and increase latency in data enrichment processes."
  annotations: {}
  severity: critical

- name: "[UNIFIED PIPELINE]: Detected higher amount of processing lag than expected."
  query: sum(sum_over_time(kafka_consumergroup_lag{consumergroup="unified-pipeline-group"}[$__range]))
  operator: gt
  threshold: [5000000]
  category: "Processing"
  frequency: 5m
  interval: 60m
  labels: 
    alert_code: "PROCESS_UNIFIED_PIPELINE_013"
  description: "A large amount of data is still waiting to be processed. This may cause in querying the most recent data."
  annotations: {}
  severity: critical