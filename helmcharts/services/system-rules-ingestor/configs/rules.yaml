- name: "High CPU Usage Detected: System Under Heavy Load"
  alertName: "High CPU Usage Detected: System Under Heavy Load"
  query: 'max without(label_system_infra, pod) (( max by (pod) (rate(container_cpu_usage_seconds_total[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="cpu"})) * 100)* on (pod) group_left (label_system_infra)kube_pod_labels{label_system_infra="true"})'
  operator: gt
  threshold: [90]
  category: "Infra System"
  frequency: "2m"
  interval: "1m"
  labels:
    dataset: all
  description: "High CPU usage (>90%) detected in ingestion system. Review the usage."
  annotations:
    causes: "1. The system is running out of resources due to Insufficient resource allocation\n2. Autoscaling might be not enabled\n3. A high volume of data might be processed and queried."
    action: "1. Manually allocate more CPU if necessary.\n2. Enable auto-scaling if it is not enable.\n3. Identify the root cause of why the service is consuming high CPU\n4. Monitor CPU usage closely.\n5. For further assistance, contact administrative support."
  severity: critical

- name: "High Memory Usage Detected: System Could Become Unstable"
  alertName: "High Memory Usage Detected: System Could Become Unstable"
  query: 'max without(label_system_infra, pod) (( max by (pod) (avg_over_time(container_memory_usage_bytes[$__range]) / on (pod) group_left max by (pod) (kube_pod_container_resource_limits{resource="memory"})) * 100)* on (pod) group_left (label_system_infra)kube_pod_labels{label_system_infra="true"})'
  operator: gt
  threshold: [90]
  category: "Infra System"
  frequency: "2m"
  interval: "1m"
  labels:
    dataset: all
  description: "High Memory usage (>90%) detected in the system. Review the usage."
  annotations:
    causes: "1. The system is running out of resources due to Insufficient resource allocation\n2. Autoscaling might be not enabled\n3. A high volume of data might be processed and queried."
    action: "1. Allocate more memory if necessary or enable auto-scaling if it is not enabled.\n2. Monitor the memory usage closely.\n3. Identify the root cause of why the service is consuming high memory.\n4. For more assistance, contact the administrative support."
  severity: critical

- name: "Node Storage is Full in Kubernetes Cluster"
  alertName: "Node Storage is Full in Kubernetes Cluster"
  query: 'min by (instance) (max by (instance, mountpoint) (max_over_time(node_filesystem_avail_bytes{mountpoint!~".*/tmp.*"}[$__range]) > 0) / on (instance, mountpoint) max by (instance, mountpoint) (node_filesystem_size_bytes{mountpoint!~".*/tmp.*"}) * 100)'
  operator: gt
  threshold: [90]
  category: "Infra System"
  frequency: "2m"
  interval: "1m"
  labels:
    dataset: all
  description: "Node storage usage is above 90% in the cluster. Review the usage."
  annotations:
    cause: "1. Historical image data may not have been cleaned up.  \n2. The temp directory might be full"
    action: "1. Check the disk usage on the node\n2. Verify image auto cleaner service is enabled or not.\n3. For more assistance, contact the administrative support"
  severity: critical

- name: "High Disk Usage Detected"
  alertName: "High Disk Usage Detected in the System"
  query: 'min by (instance) (max by (instance, mountpoint) (max_over_time(node_filesystem_avail_bytes{mountpoint!~".*/tmp.*"}[$__range]) > 0) / on (instance, mountpoint) max by (instance, mountpoint) (node_filesystem_size_bytes{mountpoint!~".*/tmp.*"}) * 100)'
  operator: gt
  threshold: [90]
  category: "Ingestion System"
  frequency: "2m"
  interval: "1m"
  labels:
    dataset: all
  description: "Disk usage is above 90% in the Ingestion system. Review the usage."
  annotations:
    cause: "1. The system is running out of disk space\n2. Volume autoscaling might be disabled\n3. Volume autoscaling may have failed due to threshold limits, subject to cloud provider limitations on scaling frequency\n4. A high volume of data is being written to the persistent storage\n5. A lot of old or unused data is accumulated in the PV"
    action: "1. Enable the auto scaling of volume\n2. Increase the volume size of the kafka system\n3. Adjust the volume autoscaler to a higher threshold percentage. This proactive measure will increase volume at a greater utilization level, preventing frequent scaling\n4. Increase the PV size if needed\n5. For more assistance, contact the adminstrative support"
  severity: critical

- name: "A high number of open connections to PostgreSQL has been detected."
  alertName: "A high number of open connections to PostgreSQL has been detected."
  query: pg_stat_activity_count
  operator: gt
  threshold: [30]
  category: "Processing System"
  frequency: "2m"
  interval: "1m"
  labels:
    dataset: all
  description: "The number of open connections to PostgreSQL has exceeded the threshold. Review the connections."
  annotations:
    cause: "1. Open connections may not have been closed\n2. Functional issues may be present\n3. The external system may have accessed the metadata storage"
    action: "1. Monitor the connection count in the database\n2. Restart PostgreSQL service if connections remain stuck\n3. For more assistance, contact the administrative support"
  severity: critical

- name: "Metadata queries are running slower than expected"
  alertName: "Metadata queries are running slower than expected"
  query: pg_stat_activity_max_tx_duration
  operator: gt
  threshold: [300]
  category: "Processing System"
  frequency: "2m"
  interval: "1m"
  labels:
    dataset: all
  description: "Metadata queries are taking longer than expected to complete. Performance degradation detected."
  annotations:
    cause: "1. Queries are stuck in a wait state due to locks held by other transactions\n2. An excessive number of simultaneous transactions is slowing down the system\n3. The database server is experiencing CPU or memory shortages, impacting performance"
    action: "1. Monitor database for long-running queries\n2. Identify and resolve blocked queries\n3. For more assistance, contact the administrative support"
  severity: critical

- name: "Storage volume resize request not processed by autoscaler"
  alertName: "Storage volume resize request not processed by autoscaler"
  query: 'count(kube_persistentvolumeclaim_info) - min(volume_autoscaler_num_valid_pvcs)'
  operator: gt
  threshold: [0]
  category: "Storage System"
  frequency: "2m"
  interval: "1m"
  labels:
    dataset: all
  description: "Volume autoscaler is not processing resize requests for some Persistent Volumes. Review the configuration."
  annotations:
    cause: "1. Persistent Volume(PV) Resizing is ignored by the Volume Autoscaler\n2. The available storage in the cluster is insuffiecient to resize PV\n3. The volume autoscaler might not be enabled"
    action: "1. Review the ignored PVs and verify if they are intentionally excluded from auto-scaling by auto scaler\n2. Check if autoscaler is enabled and configured properly\n3. Ensure there is enough free storage in the cluster\n4. For more assistance, contact the administrative support"
  severity: critical
