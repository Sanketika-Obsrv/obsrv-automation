global:
  ssl_enabled: &ssl_enabled true
  cloud_storage_provider: &cloud_storage_provider "aws"
  cloud_store_provider: &cloud_store_provider "s3"
  cloud_storage_region: &cloud_storage_region "<fill-value>" # Update the region to deploy obsrv instance

  dataset_api_container: "<fill-value>"  # Update the dataset APIs bucket name
  config_api_container: "<fill-value>"   # Update the config API's bucket name only when deploying the config-api
  postgresql_backup_cloud_bucket: &backups_bucket "<fill-value>" # Update with the backup bucket name
  velero_backup_cloud_bucket: &velero_backup_cloud_bucket "<fill-value>" # Update with velero backup  bucket name
  cloud_storage_bucket: &cloud_storage_bucket "<fill-value>" # Update with the name of cloud storage bucket
  hudi_metadata_bucket: &hudi_metadata_bucket "s3a://<fill-value>/hudi" # Update with hudi bucket name

  # Update the configuration by replacing 'name' with the access key, 'key' with the secret key, and 'region-name' with the specific region for access.
  cloud_storage_config: |+
    '{"identity":"name","credential":"key","region":"region-name"}'

  spark_cloud_bucket: ""
  storage_class_name: &storage_class_name "gp3"
  spark_service_account_arn: &spark_service_account_arn "<fill-value>" # Update with the ARN of the IAM role for this service account
  checkpoint_bucket: &checkpoint_bucket "s3://<fill-value>" # Update with checkpoint bucket name
  deep_store_type: &deep_store_type "s3"
  s3_access_key: &s3-access-key "<fill-value>" # Update with the AWS access key
  s3_secret_key: &s3-secret-access-key "<fill-value>" # Update with the AWS secret key
  s3_endpoint_url: &s3-endpoint-url ""
  kong_annotations: &kong_annotations
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
    service.beta.kubernetes.io/aws-load-balancer-eip-allocations: "<fill-value>" # Update with the Elastic IP allocation
    service.beta.kubernetes.io/aws-load-balancer-subnets: "<fill-value>" # Update with the public subnet ID

service_accounts:
  enabled: &create_sa true
  secor: &secor_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>"  # Update with the ARN of the IAM role for this service account
  dataset_api: &dataset_api_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  config_api: &config_api_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account only when deploying the config-api
  druid_raw: &druid_raw_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  flink: &flink_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  postgresql_backup: &postgresql_backup_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  s3_exporter: &s3_exporter_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  spark: &spark_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  velero: &velero_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account

s3-exporter-service-account: &s3-exporter-service-account
  s3_region: *cloud_storage_region
  serviceAccount:
    create: *create_sa
    name: s3-exporter-sa
    annotations:
      <<: *s3_exporter_sa_annotation

velero-backup: &velero-backup
  serviceAccount:
    server:
      create: *create_sa
      name: velero-backup-sa
      annotations:
        <<: *velero_sa_annotation 
      labels:
  credentials:
    useSecret: false
  configuration:
    backupStorageLocation:
    - name:
      provider: *cloud_storage_provider
      bucket: *velero_backup_cloud_bucket
      config:
        region: *cloud_storage_region
    volumeSnapshotLocation:
      - name:
        provider: *cloud_storage_provider
        config:
          region: *cloud_storage_region
  initContainers:
    - name: velero-plugin-for-aws
      image: velero/velero-plugin-for-aws
      volumeMounts:
        - name: plugins
          mountPath: /target

spark:
  serviceAccount:
    create: *create_sa
    name: spark-sa
    annotations:
      <<: *spark_sa_annotation
  persistence:
    masterTmp:
      storageClassName: *storage_class_name
    workerTmp:
      storageClassName: *storage_class_name
    masterMetadata:
      storageClassName: *storage_class_name
    workerMetadata:
      storageClassName: *storage_class_name
  cloud_provider: *cloud_storage_provider

masterdata-indexer-cron:
  serviceAccount:
    name: spark-sa
    annotations:
      <<: *spark_sa_annotation
  serviceAccountRoleArn: *spark_service_account_arn  
  dataproductsJar: "<fill-value>" # Update with the path to the data products jar

dataset-api-service-account: &dataset-api-service-account
  serviceAccount:
    create: *create_sa
    name: dataset-api-sa
    annotations:  
     <<: *dataset_api_sa_annotation


velero:
  <<: *velero-backup

s3-exporter:
  <<: *s3-exporter-service-account

dataset-api:
  <<: *dataset-api-service-account

secor-service-account: &secor-service-account
  serviceAccount:
    create: *create_sa
    name: secor-sa
    annotations:
      <<: *secor_sa_annotation


secor:
  cloud_store_provider: *cloud_store_provider
  s3_bucket_name: *backups_bucket
  s3_region: *cloud_storage_region
  storageClass: *storage_class_name
  <<: *secor-service-account

kong:
  proxy:
    annotations:
      <<: *kong_annotations
    type: LoadBalancer

flink-service-account: &flink-service-account
  serviceAccount:
    create: *create_sa
    name: flink-sa
    annotations:
      <<: *flink_sa_annotation

flink:
  <<: *flink-service-account
  checkpoint_store_type: *cloud_store_provider
  checkpointing:
    enabled: true
    statebackend: *checkpoint_bucket

postgres-backup-service-account: &postgres-backup-service-account
  serviceAccount:
    create: *create_sa
    name: postgresql-backup-sa
    annotations:
      <<: *postgresql_backup_sa_annotation

postgresql-backup:
  <<: *postgres-backup-service-account

druid-raw-service-account: &druid-raw-service-account
  serviceAccount:
    create: *create_sa
    name: druid-raw-sa
    annotations:
      <<: *druid_raw_sa_annotation

druid-raw-cluster:
  druid_deepstorage_type: *deep_store_type
  druid_indexer_logs_type: *deep_store_type
  storageClass: *storage_class_name
  s3_bucket: *cloud_storage_bucket
  s3_access_key: ""
  s3_secret_key: ""
  <<: *druid-raw-service-account

lakehouse-connector:
  hadoop_core_site:
      fs.s3a.imp: org.apache.hadoop.fs.s3a.S3AFileSystem
      fs.s3a.access.key: *s3-access-key
      fs.s3a.secret.key: *s3-secret-access-key
  serviceAccount:
    create: false
    name: flink-sa
  checkpointing:
    enabled: true
    statebackend: *hudi_metadata_bucket
    annotations:
      <<: *flink_sa_annotation

trino:
  additionalCatalogs:
    lakehouse: |-
      connector.name=hudi
      hive.metastore.uri=thrift://hudi-hms.hms.svc.cluster.local:9083
      hive.s3.aws-access-key=<fill-value>
      hive.s3.aws-secret-key=<fill-value>
      hive.s3.ssl.enabled=false

hms:
  envVars:
    WAREHOUSE_DIR: *hudi_metadata_bucket


  hadoop_core_site:
    fs.s3a.access.key:  *s3-access-key
    fs.s3a.secret.key:  *s3-secret-access-key
    # fs.s3a.endpoint: *s3-endpoint-url
    # fs.s3a.path.style.access: false
    # fs.s3a.connection.ssl.enabled: true

letsencrypt-ssl:
  enabled: *ssl_enabled

keycloak:
  ingress:
    tls: *ssl_enabled
  tls:
    enabled: *ssl_enabled  